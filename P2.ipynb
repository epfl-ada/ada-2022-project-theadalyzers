{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f0b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12a423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for reading all HTLM files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd6a6e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m wp_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ginevralarroux/Desktop/EPFL courses/Applied data analysis/ADA project/data/wpcd/wp/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# to adapt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m directories \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mpath\u001b[49m)\n\u001b[1;32m      6\u001b[0m html_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m directories:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "wp_path = \"/Users/ginevralarroux/Desktop/EPFL courses/Applied data analysis/ADA project/data/wpcd/wp/\" # to adapt\n",
    "directories = os.listdir(path)\n",
    "\n",
    "html_list = []\n",
    "for dir in directories:\n",
    "    local_path = path + dir\n",
    "    local_dir = os.listdir( local_path )\n",
    "    for link in local_dir:\n",
    "        if link[-3:]=='htm':\n",
    "            html_list.append(dir+'/'+link)\n",
    "        \n",
    "html_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4abdba1",
   "metadata": {},
   "source": [
    "The aim of this specific game strategy analysis is to test whether Wikispeedia players have clikability preferences on the positioning of the hyperlinks in the article text. Firstly we will address their postioning in the text, evaluated on the basis of the ordinal number of the paragraph they show up in; secondly we will test if there is a correlation between the clickthrough rate of the hyperlinks and their location in image captions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367618fb",
   "metadata": {},
   "source": [
    "It is therefore essential to extract from the data the clicktrhough rate for each hyperlink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for extracting the clickthrough rate of hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder='/Users/ginevralarroux/Desktop/EPFL courses/Applied data analysis/ADA project/data/wikispeedia_paths-and-graph/'\n",
    "\n",
    "paths_finished='paths_finished.tsv'\n",
    "articles='articles.tsv'\n",
    "\n",
    "paths_finished=pd.read_csv(folder+paths_finished, sep='\\t', skiprows=15, header=None, names=['hashedIpAddress', 'timestamp', 'durationInSec', 'path', 'rating'])\n",
    "articles=pd.read_csv(folder+articles, sep='\\t', skiprows=12, header=None, names=['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b35d62",
   "metadata": {},
   "source": [
    "This dataframe contains the paths chosen by the players who could reach the target article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e72681",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_finished.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00681d55",
   "metadata": {},
   "source": [
    "This dataframe lists all the clickable articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d25afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_article_name(article_column):\n",
    "    article_column=article_column.str.lower()\n",
    "    article_column=article_column.str.replace('_', ' ')\n",
    "    return article_column\n",
    "\n",
    "articles.article=filter_article_name(articles.article)\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the list articles in the lists of paths\n",
    "    \n",
    "clicked_articles=[]\n",
    "for x in paths_finished['path']: # hypothesize also in paths_unfinished\n",
    "    path=(x.split(';'))\n",
    "    clicked_articles.append(path)\n",
    "\n",
    "#for path in clicked_articles:\n",
    "#    path_series=pd.Series(path)\n",
    "#    clicked_articles=filter_article_name(path_series)\n",
    "\n",
    "clicked_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the dataframe containing the hyperlinks and their clicktrhough rates\n",
    "\n",
    "links_name=[]\n",
    "links_freq=[]\n",
    "\n",
    "i=0\n",
    "for article in articles['article']:\n",
    "    count=0\n",
    "    links_name.append(article)\n",
    "    for i in range(len(clicked_links)):\n",
    "        if article in clicked_links[i]:\n",
    "            count+=1 \n",
    "    links_freq.append(count)\n",
    "\n",
    "links_name=pd.Series(links_name)\n",
    "links_freq=pd.Series(links_freq)\n",
    "links_freq_df=pd.concat([links_name, links_freq], axis=1, names=['link', '#_clicks'])\n",
    "links_freq_df.columns=['link', '#_clicks']\n",
    "links_freq_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675681ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for extracting the positioning of the hyperlinks in the article text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8883c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af0dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = '/Users/ginevralarroux/Desktop/EPFL courses/Applied data analysis/ADA project/data/wpcd/wp/a/A_cappella.htm'\n",
    "\n",
    "f=open(URL, 'r')\n",
    "soup = BeautifulSoup(f, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aae5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the files\n",
    "soups=[]\n",
    "for html in html_list:\n",
    "    f=open(wp_path+html, 'r')\n",
    "    soup = BeautifulSoup(f, 'lxml')\n",
    "    soups.append(soup)\n",
    "    \n",
    "soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35740571",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = soup.find_all('p')\n",
    "print('The total number of paragraphs is {0}.'.format(len(paragraphs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181db7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c894c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# HTML tag for hyperlinks is <a href= >\n",
    "# this is the dataframe of the hyperlinks contained in an article, with the number of paragraph they show up in \n",
    "# and its relative postion in the article\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "\n",
    "links=[]\n",
    "i=0\n",
    "\n",
    "for p in paragraphs:\n",
    "    i+=1\n",
    "    all_links=p.find_all('a')\n",
    "    for link in all_links:\n",
    "        if 'href' in link.attrs:\n",
    "            link_title=link.text\n",
    "            link_paragraph=i\n",
    "            links.append([link_title, link_paragraph])\n",
    "\n",
    "link_positioning_df=pd.DataFrame(links, columns=['link', '#_paragraph']).drop_duplicates()\n",
    "link_positioning_df['positioning']=round(link_positioning_df['#_paragraph']/len(paragraphs), 2)\n",
    "link_positioning_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for finding hyperlinks in images' caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a372cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = '/Users/ginevralarroux/Desktop/EPFL courses/Applied data analysis/ADA project/data/wpcd/wp/a/Aachen.htm'\n",
    "f=open(URL, 'r')\n",
    "soup2 = BeautifulSoup(f, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7539e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in an HTML file the images can be either find under tables or div tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = soup2.find_all('table')\n",
    "print('The total number of tables is {0}.'.format(len(tables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb418c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = soup2.find_all('img')\n",
    "print('The total number of images is {0}.'.format(len(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1060933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images under table tags\n",
    "# dataframe of hyperlinks in tables' captions\n",
    "\n",
    "links=[]\n",
    "\n",
    "for table in tables:\n",
    "    if (table.img):\n",
    "        all_links= table.find_all('a')\n",
    "        for link in all_links:\n",
    "            if 'class' not in link.attrs:\n",
    "                link_title=link.text\n",
    "                links.append(link_title)\n",
    "        \n",
    "links_in_table_df=pd.DataFrame(links)\n",
    "links_in_table_df.columns=['link_in_table']\n",
    "links_in_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7beaa5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# images under div tags\n",
    "# dataframe of hyperlinks in tables' captions\n",
    "\n",
    "div = soup2.find_all('div')\n",
    "\n",
    "links=[]\n",
    "\n",
    "for d in div:\n",
    "    if (d.find('a', class_=\"internal\")):\n",
    "        if (d.find('img', class_='thumbimage')):\n",
    "            caption=d.find('div', class_=\"thumbcaption\")\n",
    "            if (caption.find('a')):\n",
    "                all_links=caption.find_all('a')\n",
    "                for link in all_links:\n",
    "                    if 'class' not in link.attrs:\n",
    "                        link_title=link.text\n",
    "                        links.append(link_title)\n",
    "                            \n",
    "links_in_image_df=pd.DataFrame(links).drop_duplicates() \n",
    "links_in_image_df.columns=['link_in_image']\n",
    "links_in_image_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we'll have all the articles' dataframes we will merge them by link and get the average of their positioning\n",
    "# we will sort the articles based on their clickthrough rate, ascending=False\n",
    "# we will check if there is a correlation between the positioning and clickthrough rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we'll have all the articles' dataframes we will label with a flag the hyperlinks located in images' captions\n",
    "# we will check if there is a correlation between the location in an image caption and clickthrough rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a846196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumptions: \n",
    "# we don't know for the hyperlinks that show up both in the article text and in the image caption, \n",
    "# which the player actually clicked \n",
    "# for the hyperlinks which show up multiple times in the article ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59738c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = '/Users/ginevralarroux/Desktop/EPFL courses/Applied data analysis/ADA project/data/wpcd/wp/m/Music.htm'\n",
    "\n",
    "f_music=open(URL, 'r')\n",
    "soup_music = BeautifulSoup(f_music, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96380c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_m = soup_music.find_all('p')\n",
    "print('The total number of paragraphs is {0}.'.format(len(paragraphs_m)))\n",
    "\n",
    "links_m=[]\n",
    "i=0\n",
    "\n",
    "for p in paragraphs_m:\n",
    "    i+=1\n",
    "    all_links=p.find_all('a')\n",
    "    for link in all_links:\n",
    "        if 'href' in link.attrs:\n",
    "            link_title=link.text\n",
    "            link_paragraph=i\n",
    "            links_m.append([link_title, link_paragraph])\n",
    "\n",
    "link_positioning_df_m=pd.DataFrame(links, columns=['link', '#_paragraph']).drop_duplicates()\n",
    "link_positioning_df_m['positioning']=round(link_positioning_df_m['#_paragraph']/len(paragraphs_m), 2)\n",
    "link_positioning_df_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe28480",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=link_positioning_df.merge(link_positioning_df_m, how='outer', left_on='link', right_on='link', )\n",
    "columns_to_drop=['#_paragraph_x', '#_paragraph_y']\n",
    "m=merged.drop(labels=columns_to_drop, axis=1)\n",
    "print(len(m))\n",
    "m.reset_index(drop=True, inplace=True)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_positioning=m.groupby(by=m['link']).apply(np.mean, axis=1)\n",
    "mean_positioning_df=pd.DataFrame(mean_positioning, columns=['mean_positioning'])\n",
    "mean_positioning_df.reset_index(level=1, drop=True, inplace=True)\n",
    "mean_positioning_df=mean_positioning_df.reset_index()\n",
    "mean_positioning_df\n",
    "#mean_positioning_df['link'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=links_freq_df\n",
    "\n",
    "new_data=a.merge(mean_positioning_df, left_on='link', right_on='link')\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to better take into account than the mean "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
