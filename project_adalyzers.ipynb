{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5638de80",
   "metadata": {},
   "source": [
    "# ADAlyzers final project.\n",
    "\n",
    "\n",
    "### Structure of the notebook.\n",
    "\n",
    "\n",
    "1. Introduction.\n",
    "    - What is the project about\n",
    "        - Questions, etc.\n",
    "    - How will we solve the problem?\n",
    "2. Data description + processing.\n",
    "    - General overview\n",
    "        - Size of data\n",
    "        - Types of data\n",
    "    - Data cleaning\n",
    "        - Decisions: Explain everything leading to final dataframe.\n",
    "3. Featurizing. For each item: Intro + data analysis + methods.\n",
    "    - Semantic similarity\n",
    "    - Positional encoding\n",
    "        - Images\n",
    "        - Relative to paragraph\n",
    "    - Curvature\n",
    "4. Statistical analysis.\n",
    "    - Matching\n",
    "    - Tests\n",
    "    - Conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ce788",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Introduction.\n",
    "\n",
    "Players follow different strategies.\n",
    "\n",
    "Is the implementation of some strategy in particular correlated to success in this game? Or better, can we recommend a particular strategy to play wikispeedia?\n",
    "\n",
    "Here, we will answer these questions by exploring a database of over 100k recorded games from real users, by using tools from natural language processing, web scrapping and network analysis, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ade221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18d0e618",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Data description + processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.parse\n",
    "import seaborn as sns\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8475e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/\"\n",
    "parent_folder_path = data_path + 'wikispeedia_paths-and-graph/'\n",
    "\n",
    "paths_finished_df=(pd.read_csv(os.path.join(parent_folder_path, 'paths_finished.tsv'), \n",
    "                               sep='\\t', skiprows=15, header=None)\n",
    "                   .rename(columns={0:\"ip\",\n",
    "                                    1:\"timestamp\",\n",
    "                                    2:\"duration\",\n",
    "                                    3:\"path\",\n",
    "                                    4:\"rating\"}))\n",
    "\n",
    "paths_unfinished_df=(pd.read_csv(os.path.join(parent_folder_path, 'paths_unfinished.tsv'), \n",
    "                               sep='\\t', skiprows=16, header=None)\n",
    "                     .rename(columns={0:\"ip\",\n",
    "                                      1:\"timestamp\",\n",
    "                                      2:\"duration\",\n",
    "                                      3:\"path\",\n",
    "                                      4:\"target\",\n",
    "                                      5:\"type\"}))\n",
    "articles_df=pd.read_csv(os.path.join(parent_folder_path, 'articles.tsv'), \n",
    "                        sep='\\t', skiprows=12, header=None, names=['article'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61322e7c",
   "metadata": {},
   "source": [
    "### Hyperlinks clickability: positioning throughout the text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b3b947",
   "metadata": {},
   "source": [
    "To get the distribution of players' clickability preference with regard to the positioning of the hyperlinks throughout the article text, the following protocol is implemented:\n",
    "1. parse Wikispeedia articles html files in order to get the **most frequent positioning*** per each hyperlink;\n",
    "2. parse the **human paths** (i.e. sequence of clicked articles) and attribute to each link its most frequent positioning (on the basis of the categorization above);\n",
    "3. group by **positioning**** and count the **number of occurences*****;\n",
    "4. normalize;\n",
    "5. plot the distributions of successful and unsuccessful players. \n",
    "\n",
    "\\*each article is divided into paragraphs: on the basis of the relative (to the article length) paragraph the hyperlink shows up, it is assigned a categorical label, which indicates its positioning; then, to each hyperlink name, its most frequent label, throughout the Wikispeedia library, it is assigned. <br>\n",
    "\\*\\*`positioning` is a categorical variable with possible labels `top`, `top-center`, `center`, `center-bottom`, `bottom`. <br>\n",
    "\\*\\*\\* do this separately for successful and unsuccessful players."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1bcaa9",
   "metadata": {},
   "source": [
    "`most_freq_positioning_df` stores the most frequent positionings for each hyperlink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_positioning_df=pd.read_csv('processed/most_freq_positioning_df.csv', index_col=0)\n",
    "most_freq_positioning_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d4884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e3ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from PIL import Image\n",
    "#with Image.open(\"hopper.jpg\") as im:\n",
    "    #im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d24b16",
   "metadata": {},
   "source": [
    "### Hyperlinks clickability: image captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646e907",
   "metadata": {},
   "source": [
    "To get the distribution of players' clickability preference with regard to the positioning of the hyperlinks throughout the article text, the following protocol is implemented:\n",
    "1. parse Wikispeedia articles html files in order to get the **list of hyperlinks** that show up **in image captions**;\n",
    "2. parse the **human paths** (i.e. sequence of clicked articles) and count the clicked hyperlinks that show up in image captions (on the basis of the categorization above);\n",
    "3. group by a hyperlink being in an **image captions*** or not and count the **number of occurences**;\n",
    "4. normalize;\n",
    "5. plot the distributions. \n",
    "\n",
    "\\*`in_image` is a binary variable indicating whether the hyperlink shows up in image captions or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e41f082",
   "metadata": {},
   "source": [
    "`links_in_images_list` stores the list of links present in image captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9303771",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_in_img_list=pd.read_csv('processed/links_in_images_unique.csv', index_col=0).values[:,0].tolist()\n",
    "link_in_img_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dea46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79059069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with Image.open(\"hopper.jpg\") as im:\n",
    "#    im.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39488394",
   "metadata": {},
   "source": [
    "# Dataset Processing\n",
    "\n",
    "The methodology chosen for the data processing step is to perform a **matched analysis** with the final objective to test if specific players' game strategies lead to success in the Wikispeedia game. \n",
    "A logistic regression is conducted with the goal to estimate the parameters of the logistic model with the dependent binary variable being the success (or NOT success) and the independent variables being sequentially extracted the following set of features (different players' choices when playing the Wikispeedia game):\n",
    "1. **Positioning** of the clicked hyperlinks;\n",
    "2. Clicking hyperlinks in **image captions**;\n",
    "3. Clicking articles with the following **semantic similarity patterns**:\n",
    "   -  increasing semantic similarity along the path between articles and the target article;\n",
    "   -  increasing semantic similarity along the path between two sequentially clicked articles.\n",
    "   \n",
    "Several **confounders** may affect the outcome of the regression analysis i.e. of the potentially found correlation between the aforementioned features and success. <br>\n",
    "To mention a few, the **“difficulty”** of the randomly assigned task (source and target articles), affects the players' successfulness together with the strategy adopted to address it. This “difficulty” could be naively measured by **shortest path distance** between the source and target article in the Wikispeedia hyperlinks graph (calculated with the Floyd-Warshall algorithm). <br>\n",
    "Additionally, the **characteristics** (e.g. **nodes in- and out-degree**, **path legth**) of the \"human path\" (i.e. the actual sequence of clicked hyperlinks by the player), again reasonably affects players successfulness and their choice for a strategy.  \n",
    "### Andres comment curvature features\n",
    "\n",
    "\n",
    "In order to remove the aforementioned confounders, the dataset is filtered and the matching is performed accordingly with the assumptions made. \n",
    "Players are matched on same **shortest path distance** of the assigned task, and maximum **propensity score** (see __Statistical analysis__), with the **treated subject** being the user performing a specific strategy (e.g. clicking more frequently than the average on hyperlinks showing up in images’ captions) and the **controlled subject** being the user NOT performing the strategy. \n",
    "\n",
    "The analysis challenge can be reformulated as follows: **“being assigned an equally difficult task and being equally free to perform a specific strategy, are players more successful if they adopt that specific strategy?”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79ebc",
   "metadata": {},
   "source": [
    "`shortest_path_matrix` stores the shortest path distances from all source articles to all target articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11aad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first read the shortest path matrix\n",
    "shortest_path_matrix = []\n",
    "\n",
    "with open(data_path + 'wikispeedia_paths-and-graph/shortest-path-distance-matrix.txt', 'r') as f:\n",
    "    # the first 17 lines (indexed from 0) is the file description \n",
    "    for line in f.readlines()[17:]:\n",
    "        shortest_path_matrix.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f9471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfca177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we need a list of all the article names. The order of the articles \n",
    "# is the same as the shortest_path_matrix as per the file descriptions\n",
    "\n",
    "import urllib.parse\n",
    "def str_url_format(word):\n",
    "    \"\"\"\n",
    "    Article name preprocessing.\n",
    "    \n",
    "    Apply this function any time a new dataframe is loaded.\n",
    "    \"\"\"\n",
    "    return (urllib.parse.unquote(word)\n",
    "            .replace(\"_\", \" \")\n",
    "            .strip()\n",
    "            .lower())\n",
    "\n",
    "article_names_cleaned = (pd.read_csv(data_path + 'wikispeedia_paths-and-graph/articles.tsv', \n",
    "                                     sep='\\t', \n",
    "                                     skiprows=11,\n",
    "                                     header=None)[0]\n",
    "                         .apply(str_url_format)\n",
    "                         .values\n",
    "                         .tolist()\n",
    "                        )\n",
    "\n",
    "article_names_cleaned[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a680ef",
   "metadata": {},
   "source": [
    "`successful_df` and `unsuccessful_df` dataframes store all players attempts to the Wikispeedia game. Each row contains:\n",
    "- `path`, sequence of clicked articles;\n",
    "- `source_article`, `target_article`, source and target articles of the assigned task;\n",
    "- `shortest_path_length`, shortest path distance between source and target article;\n",
    "- `human_path_length`, length of `path`. <br>\n",
    "\n",
    "The two dataframes are then filtered for `human_path_length` greater than 3 as a human path length equal to 1 would mean the player did not perform any choice and with a human path length equal to 2, no sequential behavior could be measured by us, data analysts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each human path, perform the following steps:\n",
    "#      1. extract the source and target article\n",
    "#      2. find the *index* in the article names list that corresponds to the source and target article\n",
    "#      3. the corresponding *index* row in the shortest path matrix corresponds to the source article. \n",
    "#         from this list of numbers, use the target article *index* to find the *shortest path length*\n",
    "\n",
    "def augment_with_shortest_path(df: pd.DataFrame, successful: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    this function takes a Series and returns a DataFrame with the following columns:\n",
    "       1. path\n",
    "       2. source article\n",
    "       3. target article\n",
    "       4. shortest path length\n",
    "       \n",
    "    input:\n",
    "       df: the Pandas DataFrame containing all the human navigation paths\n",
    "       successful: a boolean indicating whether the paths were successful or not\n",
    "    \"\"\"\n",
    "    # remove all paths with back-tracks\n",
    "    df = df[~df['path'].str.contains('<')]        \n",
    "    paths, human_path_lengths, source_articles, target_articles, shortest_paths = [], [], [], [], []\n",
    "    \n",
    "    # all information required for successful paths is in the path itself\n",
    "    if successful:\n",
    "        for human_path in df['path']:\n",
    "\n",
    "            split_path = str_url_format(human_path).split(\";\")\n",
    "            \n",
    "            paths.append(\";\".join(split_path))\n",
    "            # subtract 1 because we do not count the source article\n",
    "            human_path_lengths.append(len(split_path)-1)\n",
    "\n",
    "            source = split_path[0]\n",
    "            target = split_path[-1]\n",
    "            source_articles.append(source)\n",
    "            target_articles.append(target)\n",
    "    \n",
    "    # unsuccessful paths require extraction of the target article from a separate column\n",
    "    else:\n",
    "        for human_path, target in zip(df['path'], df['target']):\n",
    "            \n",
    "            split_path = str_url_format(human_path).split(\";\")\n",
    "            paths.append(\";\".join(split_path))\n",
    "            # subtract 1 because we do not count the source article\n",
    "            human_path_lengths.append(len(split_path)-1)\n",
    "\n",
    "            source = split_path[0]\n",
    "            target = str_url_format(target)\n",
    "            source_articles.append(source)\n",
    "            target_articles.append(target)\n",
    "        \n",
    "    for source, target in zip(source_articles, target_articles):\n",
    "        source_index = article_names_cleaned.index(source)\n",
    "        # there are target articles that were not provided in the plain text files\n",
    "        try:\n",
    "            target_index = article_names_cleaned.index(target)\n",
    "        except Exception:\n",
    "            shortest_paths.append(\"N/A\")\n",
    "            continue\n",
    "            \n",
    "        # query the shortest path matrix to get the correct vector (corresponding to the source article)\n",
    "        shortest_path_vector = shortest_path_matrix[source_index]\n",
    "        # now find the target article indexed integer in the vector\n",
    "        shortest = shortest_path_vector[target_index]\n",
    "        # it's not always possible to get to the target article. Impossible navigation is denoted by \"_\"\n",
    "        if shortest == \"_\":\n",
    "            shortest_paths.append(\"Impossible\")\n",
    "        else:\n",
    "            shortest_paths.append(int(shortest))\n",
    "        \n",
    "        \n",
    "    # create the augmented DataFrame\n",
    "    out = pd.DataFrame({\n",
    "                      'path': paths,\n",
    "                      'source_article': source_articles,\n",
    "                      'target_article': target_articles,\n",
    "                      'human_path_length': human_path_lengths,\n",
    "                      'shortest_path_length': shortest_paths\n",
    "                      })\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da9094",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_df = augment_with_shortest_path(df=paths_finished_df, successful=True)\n",
    "# to investigate human behaviour, we remove all \"Impossible paths\" and also shortest_path_length = 0\n",
    "successful_df = successful_df[(successful_df['shortest_path_length'].apply(lambda x: x != 'Impossible' and x != 0))]\n",
    "\n",
    "successful_df = successful_df[(successful_df['human_path_length'].apply(lambda x: x >= 3))]\n",
    "\n",
    "successful_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsuccessful_df = augment_with_shortest_path(df=paths_unfinished_df, successful=False)\n",
    "\n",
    "# some target articles for unsuccessful paths were not provided in the plain_text folder, denoted in\n",
    "# the DataFrame as \"N/A\". Remove these\n",
    "unsuccessful_df = (unsuccessful_df[(unsuccessful_df['shortest_path_length']\n",
    "                                    .apply(lambda x: x != 'N/A' and x != 'Impossible'))])\n",
    "\n",
    "unsuccessful_df = unsuccessful_df[(unsuccessful_df['human_path_length'].apply(lambda x: x >= 3))]\n",
    "\n",
    "unsuccessful_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This replaces the old plots. more concise xx\n",
    "\n",
    "\n",
    "# take a look at the shortest_path distributions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(2,1, \n",
    "                       figsize=(10,8), \n",
    "                       gridspec_kw={\"hspace\":0.4})\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "\n",
    "successful_counts = (successful_df['shortest_path_length']\n",
    "                     .value_counts()\n",
    "                     .reset_index())\n",
    "successful_counts[\"success\"] = \"successful\"\n",
    "\n",
    "unsuccessful_counts = (unsuccessful_df['shortest_path_length']\n",
    "                       .value_counts()\n",
    "                       .reset_index())\n",
    "unsuccessful_counts[\"success\"] = \"unsuccessful\"\n",
    "\n",
    "all_counts = pd.concat([successful_counts, \n",
    "                        unsuccessful_counts], \n",
    "                       axis=0)\n",
    "\n",
    "sns.barplot(data=all_counts, \n",
    "            x=\"index\", \n",
    "            y=\"shortest_path_length\", \n",
    "            hue=\"success\",\n",
    "            ax=ax[0], \n",
    "            palette=[\"#b2df8a\", \"#1f78b4\"])\n",
    "\n",
    "ax[0].set_ylabel('Absolute Counts'); \n",
    "ax[0].set_xlabel('Shortest Path');\n",
    "ax[0].set_title(\"1. Optimal lengths of the proposed games.\")\n",
    "#Count of human paths, stratified by shortest possible path\")\n",
    "\n",
    "ax[0].legend(loc=1)\n",
    "ax[0].set_ylabel('Absolute Counts') \n",
    "ax[0].set_xlabel('Shortest Path')\n",
    "\n",
    "\n",
    "\n",
    "# Next plot: frequency by human length\n",
    "n = 4\n",
    "successful_human_path_lengths_n = (successful_df\n",
    "                                   [successful_df['shortest_path_length'] == n]\n",
    "                                   ['human_path_length']\n",
    "                                   .value_counts()\n",
    "                                   .reset_index())\n",
    "\n",
    "successful_human_path_lengths_n[\"success\"] = \"successful\"\n",
    "\n",
    "unsuccessful_human_path_lengths_n = (unsuccessful_df\n",
    "                                     [unsuccessful_df['shortest_path_length'] == n]\n",
    "                                     ['human_path_length']\n",
    "                                     .value_counts()\n",
    "                                     .reset_index())\n",
    "\n",
    "unsuccessful_human_path_lengths_n[\"success\"] = \"unsuccessful\"\n",
    "\n",
    "all_path_ln = pd.concat([successful_human_path_lengths_n,\n",
    "                         unsuccessful_human_path_lengths_n])\n",
    "\n",
    "sns.barplot(data=all_path_ln, \n",
    "            x=\"index\", \n",
    "            y=\"human_path_length\", \n",
    "            hue=\"success\", \n",
    "            ax=ax[1],\n",
    "            palette=[\"#b2df8a\", \"#1f78b4\"])\n",
    "\n",
    "ax[1].set_xlim(-0.5,10.5)\n",
    "\n",
    "ax[1].set_ylabel('Absolute Counts')\n",
    "ax[1].set_xlabel('Human path length')\n",
    "ax[1].set_title(f\"2. Path lengths obtained by human players, when optimal length is {n}.\")\n",
    "# Count of human path lengths, with shortest possible path = n\");\n",
    "\n",
    "ax[1].legend(loc=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04078a7c",
   "metadata": {},
   "source": [
    "The majority of tasks have shortest path length equal to 3, among both successful and unsuccessful players. For this shortest pasth distance, the distribution of successful vs unsuccessful players is strongly unbalanced; for shortest path distance equal to 4, instead, the gap is roughly 7 times smaller, i.e. the distributions are more balanced. In the former scenario, the value of shortest path distance of the task (a naive measure of \"difficulty\") may be the underlying motivation of the uneven distribution. However it is of more interest for the purpose of our analysis to deepen the inquiry on the **more balanced dataset**, as we still have **large amount of data** to test.\n",
    "\n",
    "The dataframes are therefore filtered for `shortest_path_distance`==4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b8e86",
   "metadata": {},
   "source": [
    "The binary variable `is_successful` accounting for players' success in the game is added to the dataframes, before concatting them into the `final_df` dataframe, ready for **Featurization** and **Statistical analysis** steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bba924",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_df['is_successful'] = 1\n",
    "unsuccessful_df['is_successful'] = 0\n",
    "\n",
    "# merge the successful and unsuccessful DataFrames\n",
    "final_df = pd.concat([successful_df, unsuccessful_df])\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "final_df.head(3)\n",
    "\n",
    "# save the DataFrame\n",
    "final_df.to_csv('processed/final_df.csv', index=False)\n",
    "\n",
    "# next, we will calculate \"treatments\" involving semantic distance metrics for the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7529d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe6f5fd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d5e18",
   "metadata": {},
   "source": [
    "### Human paths characteristics: length, curvature\n",
    "\n",
    "The features holding the information on **human paths characteristics** are extrapolated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4fc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph \n",
    "\n",
    "df_links = (pd.read_csv(data_path + \"wikispeedia_paths-and-graph/links.tsv\", \n",
    "                        skiprows=12, \n",
    "                        sep=\"\\t\", \n",
    "                        header=None)\n",
    "            .applymap(str_url_format)\n",
    "            .rename(columns={0:\"src\",\n",
    "                             1:\"tgt\"})\n",
    "           )\n",
    "\n",
    "def split_decode_path(path):\n",
    "    return list(map(\n",
    "        str_url_format, \n",
    "        path.split(\";\")\n",
    "    ))\n",
    "\n",
    "df_paths_success = (pd.read_csv(data_path + \"wikispeedia_paths-and-graph/paths_finished.tsv\", \n",
    "                          skiprows=15, \n",
    "                          sep=\"\\t\", \n",
    "                          header=None)\n",
    "                    .rename(columns={0:\"ip\",\n",
    "                                     1:\"timestamp\",\n",
    "                                     2:\"duration\",\n",
    "                                     3:\"path\",\n",
    "                                     4:\"rating\"})\n",
    "         )\n",
    "# Create a \"target\" col\n",
    "df_paths_success[\"target\"] = (df_paths_success[\"path\"]\n",
    "                              .apply(lambda x: (str_url_format(x)\n",
    "                                                .split(\";\")[-1])))\n",
    "df_paths_success[\"type\"] = \"win\"\n",
    "\n",
    "\n",
    "df_paths_fail = (pd.read_csv(data_path + \"wikispeedia_paths-and-graph/paths_unfinished.tsv\", \n",
    "                             skiprows=16, \n",
    "                             sep=\"\\t\", \n",
    "                             header=None)\n",
    "                 .rename(columns={0:\"ip\",\n",
    "                                  1:\"timestamp\",\n",
    "                                  2:\"duration\",\n",
    "                                  3:\"path\",\n",
    "                                  4:\"target\",\n",
    "                                  5:\"type\"})\n",
    "             )\n",
    "\n",
    "# Join dfs to get a full df with all paths\n",
    "df_paths = pd.concat([df_paths_success,\n",
    "                      df_paths_fail])\n",
    "\n",
    "# Get path list\n",
    "df_paths[\"pathl\"] = df_paths[\"path\"].apply(split_decode_path)\n",
    "# Decode target from url\n",
    "df_paths[\"target\"] = df_paths[\"target\"].apply(str_url_format)\n",
    "# Get starting node\n",
    "df_paths[\"start\"] = df_paths[\"pathl\"].apply(lambda x:x[0])\n",
    "\n",
    "df_paths.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a687a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dfs that count node properties:\n",
    "# in-degree, out-degree, use as start, use as target\n",
    "\n",
    "freq_out = df_links['src'].value_counts()\n",
    "freq_in = df_links['tgt'].value_counts()\n",
    "\n",
    "degree_freq = (pd.merge(freq_in,\n",
    "                        freq_out, \n",
    "                        left_index=True, \n",
    "                        right_index=True, \n",
    "                        how='outer')\n",
    "               .fillna(0)\n",
    "               .rename(columns={\"src\":\"out\",\n",
    "                                \"tgt\":\"in\"}))\n",
    "\n",
    "degree_freq.head()\n",
    "\n",
    "from itertools import chain\n",
    "degree_connect = (\n",
    "    degree_freq\n",
    "    .merge(\n",
    "        # Count how many times an article was used as target\n",
    "        df_paths[\"target\"].value_counts(),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"outer\"\n",
    "    )\n",
    "    .merge(\n",
    "        # Count how many times an article was used as starting point\n",
    "        df_paths[\"start\"].value_counts(), \n",
    "        left_index=True, \n",
    "        right_index=True,\n",
    "        how=\"outer\")\n",
    "    .merge(\n",
    "        # Count how many times an article was clicked\n",
    "        pd.Series(\n",
    "            list(chain(\n",
    "                *df_paths[\"pathl\"].values\n",
    "            )), \n",
    "            name=\"clicks\"\n",
    "        )\n",
    "        .value_counts(), \n",
    "        left_index=True, \n",
    "        right_index=True,\n",
    "        how=\"outer\")\n",
    ")\n",
    "\n",
    "degree_connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate curvature for each edge in the graph\n",
    "\n",
    "# get in-degree(`in`) and out-degree(`out`)\n",
    "curv_edge = (\n",
    "    df_links\n",
    "    .merge(\n",
    "        (degree_connect[[\"in\"]]\n",
    "         .rename(columns={\"in\":\"in-degree(src)\"})),\n",
    "        left_on=\"tgt\", \n",
    "        right_index=True,\n",
    "        how=\"outer\")\n",
    "    .merge(\n",
    "        (degree_connect[[\"out\"]]\n",
    "         .rename(columns={\"out\":\"out-degree(tgt)\"})),\n",
    "        left_on=\"src\", \n",
    "        right_index=True,\n",
    "        how=\"outer\")\n",
    ")\n",
    "\n",
    "# Calculate curvature\n",
    "curv_edge[\"curvature\"] = 2 - curv_edge[\"in-degree(src)\"] - curv_edge[\"out-degree(tgt)\"]\n",
    "\n",
    "curv_edge.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD features to final df\n",
    "\n",
    "# First get, for every path, a sequence of nodes.\n",
    "\n",
    "# TODO: Think abt what to do with paths containing `<`\n",
    "\n",
    "# For convenience, let's drop paths contatining '<' for now\n",
    "df_paths_clean = (final_df[final_df[\"path\"]\n",
    "                           .apply(lambda x: '<' not in x)])\n",
    "df_paths_clean[\"pathl\"] = df_paths_clean.path.str.split(\";\")\n",
    "\n",
    "def list_nodes(pathl):\n",
    "    return [\n",
    "        [pathl[i],pathl[i+1]]\n",
    "        for i in range(len(pathl)-1)\n",
    "    ]\n",
    "    \n",
    "# Array to index edge -> curvature \n",
    "indx_curva = (curv_edge\n",
    "              .set_index([\"src\", \"tgt\"])\n",
    "              .loc[:,\"curvature\"])\n",
    "def path_curvature(row):\n",
    "    try:\n",
    "        return (indx_curva\n",
    "                .loc[row[\"edgel\"]]\n",
    "                .values)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "df_paths_clean[\"edgel\"] = df_paths_clean[\"pathl\"].apply(list_nodes)\n",
    "df_paths_clean[\"path_curv\"] = df_paths_clean.apply(path_curvature, axis=1)\n",
    "\n",
    "df_paths_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "final_df_curv = df_paths_clean.drop(columns=[\"pathl\", \"edgel\"])\n",
    "\n",
    "def curv_features(path_curv):\n",
    "    \"\"\"\n",
    "    Calculate curvature features of a path.\n",
    "    \n",
    "    With the curvature of the edges in the path, calc a fixed length vector that (approx) describes it.\n",
    "    \"\"\"\n",
    "    path_curv = pd.Series(path_curv, dtype=np.float64)\n",
    "    p_curv_diff = np.diff(path_curv)\n",
    "    \n",
    "    descr = path_curv.describe()\n",
    "    descr2 = pd.Series(p_curv_diff).describe()\n",
    "    \n",
    "    return pd.concat([descr, descr2]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "curv_feats = final_df_curv[\"path_curv\"].apply(curv_features)\n",
    "\n",
    "final_df_curv = pd.concat([final_df_curv\n",
    "                           .drop(columns=\"path_curv\"),\n",
    "                           curv_feats],\n",
    "                          axis=1)\n",
    "\n",
    "final_df_curv = final_df_curv.rename(columns={i:f\"curv_feats_{i}\" for i in range(16)})\n",
    "\n",
    "final_df_curv.to_csv(\"processed/final_df_curv_feats.csv\", index=False)\n",
    "final_df_curv.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a29448",
   "metadata": {},
   "source": [
    "### Hyperlinks' positioning\n",
    "\n",
    "The features holding the information on **hyperlinks' positioning clickability** are extrapolated as follows:\n",
    "1. to each hyperlink clicked in the human path it is assigned its most frequent positioning*;\n",
    "2. for each path the distribution of the 5 categorical positioning (top, center-top, center, center-bottom,bottom) of hyperlinks is calculated (their relative frequency: numerical predictor).\n",
    "3. the values of the features are standardized**.\n",
    "\n",
    "*its most frequent positioning is determined on the basis of its occurencies in the set of articles making up the Wikispeedia html files database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_positioning_df=pd.read_csv('processed/most_freq_positioning_df.csv', index_col=0)\n",
    "most_freq_positioning_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91eca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(human_path):\n",
    "    '''this function determines the categorical positioning feature of each hyperlink in the human path'''\n",
    "    return [most_freq_positioning_df.loc[link].values.item() \n",
    "                         if link in most_freq_positioning_df.index \\\n",
    "                         else np.random.choice(['top', 'center-top','center','center-bottom', 'bottom'])\n",
    "                         for link in human_path] #certain hyperlinks were not classified \n",
    "\n",
    "final_df_postioning = final_df_curv['path'].apply(lambda x: x.split(\";\")).apply(lambda x: find_features(x))\n",
    "\n",
    "def find_features_frequency(features):\n",
    "    '''this function counts the frequency of the categorical positioning features of the hyperlinks in each human path'''\n",
    "    t, ct, c, cb, b=[], [], [], [], []\n",
    "\n",
    "    for path in features:\n",
    "        t.append(path.count('top'))\n",
    "        ct.append(path.count('center-top'))\n",
    "        c.append(path.count('center'))\n",
    "        cb.append(path.count('center-bottom'))\n",
    "        b.append(path.count('bottom'))\n",
    "    return pd.DataFrame({'top': t, 'center_top': ct, 'center': c, 'center_bottom': cb, 'bottom': b})\n",
    "\n",
    "successful_features_freq = find_features_frequency(final_df_postioning)\n",
    "\n",
    "final_df_feats = (pd.concat([final_df_curv, successful_features_freq], \n",
    "                            axis=1)\n",
    "                  .dropna())\n",
    "final_df_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9073167d",
   "metadata": {},
   "source": [
    "### Hyperlinks in image captions\n",
    "\n",
    "The feature holding the information on **hyperlinks in image captions clickability** is extrapolated as follows:\n",
    "- for each human path the numerosity of hyperlinks showing up in images captions is counted (numerical predictor);\n",
    "- the value is standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_in_img_list=pd.read_csv('processed/links_in_images_unique.csv', index_col=0).values[:,0].tolist()\n",
    "link_in_img_list\n",
    "\n",
    "#list of unique hyperlinks showing up in images'captions\n",
    "\n",
    "#count number of hyperlinks in images' captions in each path\n",
    "\n",
    "final_df_feats['count_images'] = (final_df['path']\n",
    "                                 .apply(lambda x: x.split(\";\"))\n",
    "                                 .apply(lambda x: np.isin(x, link_in_img_list).sum()\n",
    "                                        if np.isin(x, link_in_img_list).sum() else 0)\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab12a5a",
   "metadata": {},
   "source": [
    "### Semantic similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c059a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10c034a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298f7860",
   "metadata": {},
   "source": [
    "### Calculation of propensity score \n",
    "\n",
    "The purpose motivating the matched analysis is to obtain a \"balanced\" testing dataset, with subjects actually comparable. In fact, we want to test potential correlation (causation ?) between specific strategies performed and players' success. The players tested should then be at the same starting point when choosing what strategy to adopt, i.e. have same probability to get the treatment (a.k.a. propensity score).   \n",
    "The **propensity scores** are obtained by calculating the **predicted outcomes of a logistic regression** with the dependent variable being the boolean treatment variable `treat` and the independent variables being selected features, namely:\n",
    "- human path length `human_path_length`;\n",
    "- curvature;\n",
    "- \n",
    "- \n",
    "- <br>\n",
    "\n",
    "The **treated subject** is the user performing the specific strategy, the **controlled subjct** is the user NOT performing the specific startegy. The following tretments will be tested:\n",
    "1. Players clicking in hyperlinks showing up at the top of articles;\n",
    "2. Players clicking in hyperlinks showing up at the top-center of articles;\n",
    "3. Players clicking on hyperlinks showing up in image captions;\n",
    "4. Players clicking articles with sequentially increasing similarity between the two;\n",
    "5. Players clicking articles with increasing similarity with the target article;\n",
    "\n",
    "The **label** is players' success `is_successful`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84405465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's center our attention on paths with optimum length == 4\n",
    "# Here tasks are harder, and so classes (succcess/unsuccess) are more balanced.\n",
    "# Also less data -> matching is faster\n",
    "\n",
    "final_df_filt = final_df_feats.query(\"shortest_path_length == 4\")\n",
    "final_df_filt.is_successful.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api\n",
    "import statsmodels.formula.api as smf\n",
    "from itertools import chain \n",
    "\n",
    "# Generate the features + standardize\n",
    "y_treatment = ((final_df_filt.top/final_df_filt.human_path_length) > 0.5)*1\n",
    "X_feats = (final_df_filt\n",
    "           .loc[:,final_df_filt\n",
    "                .columns\n",
    "                .str\n",
    "                .startswith(\"curv_feats\")])\n",
    "\n",
    "X_feats = (X_feats - X_feats.mean(axis=0))/X_feats.std(axis=0)\n",
    "\n",
    "# Calc prop score (w logistic regr)\n",
    "df_regr = pd.concat([X_feats, y_treatment.rename(\"treat\")], axis=1)\n",
    "\n",
    "model = smf.logit(formula='treat ~ '+ '+'.join(X_feats.columns), data=df_regr)\n",
    "res = model.fit()\n",
    "\n",
    "# Add treatment flag to df\n",
    "final_df_filt[\"treatment_top\"] = y_treatment\n",
    "\n",
    "# Add prop score to df\n",
    "final_df_filt[\"prop_scores_top\"] = res.predict()\n",
    "\n",
    "# Separate the treatment and control groups\n",
    "treatment_df = final_df_filt[final_df_filt['treatment_top'] == 1]\n",
    "control_df = final_df_filt[ final_df_filt['treatment_top'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71487a8",
   "metadata": {},
   "source": [
    "### Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d132ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate the treatment and control groups\n",
    "#treatment_df = final_df_filt[final_df_filt['treatment_top'] == 1]\n",
    "#control_df = final_df_filt[ final_df_filt['treatment_top'] == 0]\n",
    "#\n",
    "## Create an empty undirected graph\n",
    "#G = nx.Graph()\n",
    "#\n",
    "## Loop through all the pairs of instances\n",
    "#for control_id, control_row in tqdm(control_df.iterrows(), total=control_df.shape[0]):\n",
    "#    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "#\n",
    "#        # We control for task difficulty (more general in case we don't pre-filter)\n",
    "#        if control_row[\"shortest_path_length\"] == treatment_row[\"shortest_path_length\"]:\n",
    "#\n",
    "#            # Calculate the similarity \n",
    "#            similarity = get_similarity(control_row['prop_scores_top'],\n",
    "#                                        treatment_row['prop_scores_top'])\n",
    "#\n",
    "#            # Add an edge between the two instances weighted by the similarity between them\n",
    "#            G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "#\n",
    "#matching = nx.max_weight_matching(G)\n",
    "#            \n",
    "#idxs = list(chain(*matching))\n",
    "#matched_df_top = final_df_filt.loc[idxs]\n",
    "#\n",
    "#matched_df_top.to_csv('matched_df_top.csv', index=False)\n",
    "#\n",
    "#%%time \n",
    "## Generate and return the maximum weight matching on the generated graph\n",
    "#matching = nx.max_weight_matching(G)\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a1d68",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic regression to test statistical difference between the treated and control groups.\n",
    "#mod_top = smf.logit(formula = 'is_successful ~ C(treatment_top)', data=matched_df_top)\n",
    "#res = mod_top.fit()\n",
    "#print(res.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
